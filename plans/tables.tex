\section{Database Tables and Forms}
\label{sec:tables}

This section defines the fields needed for each type of information (\Element, \Tree, \etc).
Fields marked `[auto]' should be automatically populated when the contribution is saved.
All other fields should be provided by the Contributor via an input form.

I think it's realistic to settle on the set of database tables and their relationships early on.
But some of the content in the tables (columns, allowable values for some columns) will need to be adjusted as we go along.

%--------------------------------------------------
\subsection{Elements}
\label{sec:tables_element}
%--------------------------------------------------

Each \Element consists of one \Tree (\cref{sec:tables_tree}), optionally one \Trait (\cref{sec:tables_trait}), and some other information.
There should be a form for a Contributor to create a new \Element.
% The reason for \Tree and \Trait being separate entities, rather than only parts of \Element, is so they can be reused, \eg same \Tree with different \Traits makes for different \Elements.

\begin{description}
    \item[Unique ID] Arbitrary, \eg E-47295. [auto]
    \item[Date] When this was created. [auto]
    \item[Contributor] Who created this. [auto]
            Can be filled in automatically because a Contributor must be logged in to add anything new to the database (\cref{sec:users_contributor}).
    \item[Name] A brief description of the \Element.
            Free text with a maximum of 50 characters.  Might not be unique.
    \item[Comment] Any other text the Contributor would like to provide about the \Element.
            Free text.  Formatting should be preserved.
    \item[Tree] Link to one \Tree.
            An existing \Tree could be selected. %  (There will be too many Trees to present them all as a list.  Perhaps the Contributor could start to type the Tree's UniqueID or Name and the options could auto-complete.)
            Or, a new \Tree could be created along with this \Element (see \cref{sec:tables_tree}).
    \item[Trait] Link to one \Trait, or empty.
            (Some Tasks may not require a Trait in each Element.  But for our initial tasks, there will always be one.)
            An existing \Trait could be selected. %  (Again, there will be too many Traits to present them all as a list.)
            Or, a new \Trait could be created along with this \Element (see \cref{sec:tables_trait}).
            We will need a way to check that the species names agree in a \Trait and \Tree that are part of the same \Element.
    \item[Number of items] Positive integer.
            To avoid ambiguity, the Contributor will provide this.
            But the value should be sanity-checked against the number of items in the \Element's \Tree and \Trait.
            % Could get 50 elements via 50 trees and no traits, or via 1 tree and 50 traits, or via 50 trees and 50 traits.
    \item[Reference set] Link to one or more \Refsets (\cref{sec:tables_refset}) that this \Element will belong to.
    % \item[Benchmark] Link to \Benchmarks (\cref{sec:tables_benchmark}) that include this \Element, if any.
    \item[Correct answer] What answer should be obtained.
            The \Element will be associated with one or more \Tasks.
            (This is determined by the \Refsets selected above, which each map to \Tasks.)
            For each \Task, there is a correct answer that should be obtained from the \Element.
            Ideally, the Contributor would be presented with a small table showing each relevant \Task, with a box to provide the corresponding `Correct answer`.
            % This will be compared against the output of \Methods.
            % Each \Task will need to have a correct outcome defined.
\end{description}

% It will be common for a user to download one or more \Elements (\cref{sec:downloads_element}).

We will want to be able to find all the \Elements to which a \Tree or \Trait belongs.
From this, we can find all \Trees that use a \Trait, and all \Traits that go with a \Tree.

%--------------------------------------------------
\subsection{Trees}
\label{sec:tables_tree}
%--------------------------------------------------

It should only be possible to create a new \Tree as part of a new \Element.
Thus, the form for these \Tree fields should be part of the form for contributing an \Element.

Each \Tree object is actually a set of trees, all with the same properties.

\begin{description}
    \item[Unique ID] Arbitrary, \eg T-83247. [auto]
    \item[Date] When this was created. [auto]
    \item[Contributor] Who created this. [auto]
    \item[Name] A brief description of the \Tree.
            Free text with a maximum of 50 characters.  Might not be unique.
    \item[Comment] Any other text the Contributor would like to provide about the \Tree.
            Free text.  Formatting should be preserved.
    \item[Number of items] The number of unique trees uploaded.
            For safety, have the user provide this but check it against the actual trees.
    \item[Number of tips] The number of tips per tree.  Could be a single number or a range.
            For safety, have the user provide this but check it against the actual trees.
    \item[Source] A text file uploaded by the Contributor.
            Usually, this will be a script that generates the tree(s) or downloads them.
            If that is not possible, the file can contain notes explaining where the trees came from.
    \item[Tree files] Each individual tree is itself stored as a \href{http://evolution.genetics.washington.edu/phylip/newicktree.html}{Newick string}.
            Those strings could reside directly within the database; they can be quite long, though, which might be troublesome.
            Or the database entry could be a link to text file(s) containing the trees; this might be better because such files will frequently be downloaded by users (\cref{sec:downloads_element}).
            The Contributor will provide one text file per tree, so we should allow multiple files or one zipped file to be uploaded.
            % Or all the trees could be in a single text file, each on a new line.  Or NEXUS allows multiple trees per file, and also with traits.
    \item[Keywords] A list of options, from which the Contributor will choose one or more.
            We will want to be able to search/filter \Trees based on these values.
            The exact keywords will be determined as we go along, but a starting point is:
                `simulated',
                `empirical',
                `penalized likelihood'
    % \item [Tags] Various descriptive words.
    %     The idea with tags is that they are not necessarily alternatives (like `simulated' versus `empirical'), and there could be any number per \Tree.
    %     With use, we might realize that some tags can be converted to columns, and maybe vice versa.
    %     Tags probably involves two extra database tables:
    %     (1) columns are TagID and TagName, one row per tag;
    %     (2) columns are TreeID and TagID, one row per tag per tree.
\end{description}

%--------------------------------------------------
\subsection{Traits}
\label{sec:tables_trait}
%--------------------------------------------------

It should only be possible to create a new \Trait as part of a new \Element.
Thus, the form for these \Trait fields should be part of the form for contributing an \Element.

Each \Trait object consists of at least one trait value per species.
There could be multiple such sets in one \Trait object, \eg when simulating data so that each tree in a \Tree goes with one trait set in a \Trait.
% A \Trait isn't useful on its own---it only makes sense when associated with a \Tree.
% A \Trait will usually only correspond to one \Tree, but that's not required.
% For example, if empirical data are used to make different \Trees with different phylogenetic inference methods, we would have many \Trees and one \Trait.
All the trait sets within a \Trait have the same properties.

\begin{description}
    \item[Unique ID] Arbitrary, \eg A-57387. [auto]
    \item[Date] When this was created. [auto]
    \item[Contributor] Who created this. [auto]
    \item[Name] A brief description of the \Trait.
            Free text with a maximum of 50 characters.  Might not be unique.
    \item[Comment] Any other text the Contributor would like to provide about the \Trait.
            Free text.  Formatting should be preserved.
    \item[Number of items] The number of unique trait sets uploaded.
            For safety, have the user provide this but check it against the actual files.
    % \item[Number of trait sets] Positive integer.  Probably either 1, or one per tree.
    \item[Source] A text file uploaded by the Contributor.
            Usually, this will be a script that generates the trait values or downloads them.
            It might be the same as the generating script for a corresponding \Tree.
            If there is no script, the file can contain notes explaining how to get the trait values.
    \item[Traits files] Each set of traits is simply a list of numbers, labeled by tip/species name.
            As for \Trees (\cref{sec:tables_tree}), the state info could reside directly within the database or in a linked text file (\eg CSV). % which will frequently be downloaded by users (\cref{sec:downloads_element}).
            The Contributor will provide one text file per set of traits (\ie per tree), so we should allow multiple files or one zipped file to be uploaded.
            % Or all the trait values could be in a single text file, each as a column.
    \item[Keywords] A list of options, from which the Contributor will choose one or more.
            We will want to be able to search/filter \Traits based on these values.
            The exact keywords will be determined as we go along, but a starting point is:
                `empirical',
                `simulated',
                `continuous',
                `discrete',
                `binary',
                `BM',
                `OU',
                `threshold',
                `Mk',
                `SSE',
                `neutral',
                `fast',
                `slow'
\end{description}

%--------------------------------------------------
\subsection{Reference Set}
\label{sec:tables_refset}
%--------------------------------------------------

Each \Refset is a collection of \Elements (perhaps hundreds), plus some other information.

\begin{description}
    \item[Unique ID] Arbitrary, \eg R-43853.  Auto-generated when created.
    \item[Name] A short phrase that identifies the \Refset to a human.
    \item[Description] An explanation of what this \Refset is designed to test.
    \item[Elements] Link to \Element(s) in the \Refset.  It should also be easy to obtain the total number of \Elements.
    \item[Benchmarks] Link to \Benchmark(s) that contain parts of this \Refset, if any.
    \item[Methods] Link to \Method(s) for which this \Refset is relevant.
    \item[History] Notes from Contributors who have created or changed the \Refset.  (More than one column, but I'm not sure of the best format.)
\end{description}

(I'm not sure how much linking across tables is necessary.
For example, I've included links to \Benchmark and \Method here, but perhaps those connections could be obtained merely from the \Benchmark and \Method tables themselves.)

%--------------------------------------------------
\subsection{Benchmark Set}
\label{sec:tables_benchmark}
%--------------------------------------------------

Each \Benchmark is a collection of \Elements (perhaps dozens), linked to one specific \Task, plus some other information.
These may be updated frequently, as new \Elements are added and old ones are removed.

\begin{description}
    \item[Unique ID] There won't be many of them, so we could have more meaningful names.  These could be constructed from the name of the \Task and a number (in case we want to be trying out a few benchmark sets at once).
    \item[Name] A short phrase that identifies the \Benchmark to a human.
    \item[Description] An explanation of what this \Benchmark is designed to test.
    \item[Task] Link to the \Task.
    \item[Elements] Link to \Element(s) in the \Benchmark.  It should also be easy to obtain the total number of \Elements and their \Refset membership.
    \item[Methods] Link to \Method(s) for which this \Benchmark is relevant.
    \item[Correct outcome] What answer should be obtained by an effective \Method, for a particular \Task.  This will be the same for all \Elements in the \Benchmark.
    \item[History] Notes from Contributors who have created or changed the \Benchmark.  (More than one column, but I'm not sure of the best format.)
\end{description}

%--------------------------------------------------
\subsection{Task}
\label{sec:tables_task}
%--------------------------------------------------

The top layer of organization is the \Task.
There won't be many, and perhaps only one for awhile (\ie state-dependent diversification).

Not discussed in the original proposal (\cref{sec:proposal}) is the concept of grouping tasks or questions within them.
For example, if the task is state-dependent diversification, there could be different sub-tasks for discrete-valued and continuous-valued traits.
We might also want different questions for hypothesis testing (Is my trait associated with diversification shifts?) versus parameter estimation (How much higher are speciation rates for this state?).
This latter level of detail is essential for reporting results simply (\cref{sec:tables_result}).

I think designing this layer of organization well will be quite important for \phycomb to be useful.
But we may not know the best strategy until we see how it grows.
So we'll need more discussions and probably some flexibility here.
For the moment, let's assume that \Task is defined as specifically as necessary.
This suggests using only the following fields:

\begin{description}
    \item[Unique ID] There won't be many of them, so we could have meaningful names.
    \item[Name] A short phrase that identifies the \Task to a human.
\end{description}

%--------------------------------------------------
\subsection{Methods}
\label{sec:tables_method}
%--------------------------------------------------

\cref{sec:tables_element,sec:tables_tree,sec:tables_trait,sec:tables_refset,sec:tables_benchmark,sec:tables_task} were all about the testing datasets themselves.
Now we consider the analysis methods that the tests are designed to evaluate.

\begin{description}
    \item[Unique ID] Arbitrary, \eg M-93925. [auto]
    \item[Date] When this was created. [auto]
    \item[Contributor] Who created this. [auto]
    \item[Name] A brief description of the \Method.
            Free text with a maximum of 50 characters.  Might not be unique.
    \item[Comment] Any other text the Contributor would like to provide about the \Method.
            Free text.  Formatting should be preserved.
    \item[Task] Choose from the list of existing \Tasks that this \Method is be used for.
            I'm pretty sure that each \Method can only be used for a single \Task, because the \Method can only return a single value.
            The \Task therefore determine the return value type (Boolean or numeric) of the \Method.
    \item[Source] The analysis script used to run the method.  One or more text files.
    \item[Allied methods] Note any existing \Methods that are very closely related to this one.
            There may be too many \Methods to present them all as a list.  Perhaps the Contributor could start to type the UniqueID or Name and the options could auto-complete.
            (These links will need to be updated automatically, as well.  For example, when I contribute Method 2 today I can note that it is allied with Method 1 that I created yesterday.  But then the link from Method 1 to 2 should be created automatically.)
    \item[Model-based] Choose `yes' or `no'.
    \item[Keywords] A list of options, from which the Contributor will choose one or more.
            We will want to be able to search/filter \Methods based on these values (as well as on `Model-based').
            The exact keywords will be determined as we go along, but a starting point is:
                `Akaike information criterion',
                `Bayes factor',
                `bootstrap',
                `model averaging',
                `parameter estimates',
                `posterior predictive',
                `randomization test',
                `semi-parametric',
                `sign test'
\end{description}

Methods often come in groups, and I'm not sure how to handle this.
Any thoughts or suggestions appreciated.
For example, the same basic procedure could be tweaked in a few ways (in the algorithm, or the type of stats used), and testing would reveal which was best.
Some possible approaches:
\begin{itemize}
    \item Ignore this complication.
          Each \Method is a stand-alone entity, and Contributors can just explain in the Comments if they want.
    \item Ignore this complication in the database structure, but maintain by hand a high-level summary of the available \Methods.
          Could be tough if \Methods are added frequently, though could ask Contributor for suggestions on how to update the summary accordingly.
    \item Collect enough meta-data (Columns, Tags, \etc) to generate a summary of relationships among \Methods.
          Not clear that this can usefully be done algorithmically, though.
    \item Allow one \Method to link to others.
          But then get into issues of how similar is similar enough to link (\eg `coded like' versus `inspired by').
          And then there would be lots of code reuse among scripts of different-but-related \Methods.
          [note: this is the `Allied methods' field specified above]
    \item Allow multiple analysis scripts and/or libraries of shared functions per \Method.
          Seems likely to get messy, though, especially when summarizing results.
    \item Allow one explicit level of hierarchy in \Methods.
          Contributor groups them together if they share code, but each is numbered separately and results are reported separately for each variant.
\end{itemize}

% I'm currently leaning toward the by-hand summary or the single layer of explicit hierarchy.

%--------------------------------------------------
\subsection{Results}
\label{sec:tables_result}
%--------------------------------------------------

When a \Method is run on an \Element, the output should be a clear answer to a \Task.
This is a \Result.
There will actually be a value for each Item in the \Element (\eg ten numbers if there are ten individual trees)---this is what the Contributor will provide.
We'll want \phycomb to retain this level of detail so it is available for download.
But in the \phycomb web interface, we will usually want to see a summary in which each Method-Element combination is reduced to a single value, probably the proportion of Items that yielded the correct answer (see the Outcome column in \cref{fig:ui_method_performance}).

I think the best thing here will be for a Contributor to upload a CSV file with all the information needed to compute one or more \Results.
The columns should be:
\begin{itemize}
    \item Method (its Unique ID)
    \item Element (its Unique ID)
    \item Item (1, 2, 3, \ldots)
    \item Value (TRUE/FALSE or a number)
\end{itemize}

That uploaded file will need to be checked before the contents are accepted.  Validation should include:
\begin{itemize}
    \item Each \Method exists
    \item Each \Element exists
    \item All the Items for that \Element are included
    \item The Value is of appropriate type (Boolean or numeric) for the \Method and \Task
    \item The Value does not disagree with a value that was already reported
\end{itemize}

The form for uploading \Results can be minimal: just a box for CSV file(s).

The following fields should be automatically filled in when a new \Result is created.
\begin{description}
    \item[Date] When this was created. [auto]
    \item[Contributor] Who created this. [auto]
\end{description}

We will want to be able to connect each \Element with the \Methods that run on it.
The \Results table provides this link.

One concern I have is that reducing each \Result to a single number would cause analyses to be simple-minded rather than nuanced.
On the other hand, we need to keep things simple enough that we can summarize across lots of tests and methods.
Would be good to discuss this.

We will definitely need nice ways to summarize this huge table of results for users.
These summaries are called \Reports elsewhere in this document.
But I'm not sure whether this would be done within the database structure or when generating views (\eg \cref{sec:views_report_task,sec:views_report_method}).
Some summaries we'll want:
\begin{itemize}
    \item Overall performance of a \Method on a \Refset for a \Task
    \item Overall performance of a \Method on a \Benchmark for a \Task
    \item Overall performance of a \Method at a \Task
    \item Performance of all \Methods on an \Element
    \item Performance of all \Methods on an \Refset
    \item Performance of all \Methods on an \Benchmark
\end{itemize}




%--------------------------------------------------
% "Normalization rules" for relational database structure:
% https://en.wikipedia.org/wiki/Database_normalization
% 
% 1a.  Every cell contains a single value, not a list of values.
% 1b.  No repeating group of columns (like item1, item2...).  Instead, create another table with one-to-many.
% 
% 2a.  Every non-key column is fully dependent on the primary key.
% 2b   And if the primary key is made up of several columns, every non-key column depends on the entire key.
% 
% 3a.  The non-key columns don't depend on each other.
% 3b.  They depend on the (entire) primary key (rule 2), not other non-key columns.
% 
% There are more normalization rules.  But the basic idea is to think through operations and avoid potential mistakes:
%   * Adding data:   Does it need to be added in more than one place?
%   * Changing data: Could it accidentally not be changed in all places?
%   * Deleting data: Is additional information unintentionally lost?
% 
% A normalization rule could broken to improve performance, or there can be weird situations.  But breaking a rule should be intentional, well-documented, and with extra programming logic to handle it with care.
%--------------------------------------------------
